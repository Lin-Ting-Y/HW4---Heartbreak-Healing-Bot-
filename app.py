import os
from pathlib import Path
from shutil import rmtree
import streamlit as st
from dotenv import load_dotenv

# --- LangChain Imports ---
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    _HAS_LC_GOOGLE = True
    _LC_GOOGLE_ERR = None
except Exception as _e:
    _HAS_LC_GOOGLE = False
    _LC_GOOGLE_ERR = str(_e)
    ChatGoogleGenerativeAI = None  # type: ignore
    import google.generativeai as genai

# 1. ç’°å¢ƒè®Šæ•¸è¼‰å…¥èˆ‡æª¢æŸ¥
def load_env() -> str:
    load_dotenv()
    # å„ªå…ˆå¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼Œå…¶æ¬¡å¾ Streamlit Secrets (é›²ç«¯éƒ¨ç½²ç”¨)
    api_key = os.getenv("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY", "")
    
    if api_key:
        os.environ["GOOGLE_API_KEY"] = api_key
    else:
        st.warning("âš ï¸ ç¼ºå°‘ GOOGLE_API_KEYï¼Œè«‹åœ¨ .env æˆ– Streamlit secrets ä¸­è¨­å®šã€‚", icon="âš ï¸")
    return api_key

# 2. å»ºç«‹å‘é‡è³‡æ–™åº« (å¼·åˆ¶ä½¿ç”¨ CPU ç‰ˆ HuggingFaceï¼Œç©©å®šä¸”å…è²»)
def get_vector_store(books_dir: str = "books", cache_dir: str = ".faiss_index") -> FAISS:
    base_path = Path(books_dir)
    if not base_path.exists():
        base_path.mkdir(parents=True, exist_ok=True)

    # å˜—è©¦è®€å–å¿«å–
    cache_path = Path(cache_dir)
    if cache_path.exists():
        try:
            # å¼·åˆ¶æŒ‡å®š device="cpu"ï¼Œé¿å…åœ¨é›²ç«¯æ‰¾ä¸åˆ° GPU è€Œå ±éŒ¯
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={"device": "cpu"}
            )
            return FAISS.load_local(str(cache_path), embeddings, allow_dangerous_deserialization=True)
        except Exception:
            pass # è®€å–å¤±æ•—å°±é‡æ–°å»ºç«‹

    # è®€å–æ›¸ç±æª”æ¡ˆ
    loader = DirectoryLoader(
        str(base_path),
        glob="**/*.txt",
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8"}, # ç¢ºä¿ä¸­æ–‡æ­£å¸¸
        show_progress=True,
    )
    docs = loader.load()

    if not docs:
        return None

    # åˆ‡åˆ†èˆ‡å‘é‡åŒ–
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = splitter.split_documents(docs)
    
    # å»ºç«‹æ–°ç´¢å¼•
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"}
    )
    vector_store = FAISS.from_documents(chunks, embeddings)
    
    # å„²å­˜å¿«å–
    cache_path.mkdir(parents=True, exist_ok=True)
    vector_store.save_local(str(cache_path))
    return vector_store

# 3. å»ºç«‹ AI äººè¨­ Prompt
def build_persona_prompt(context: str) -> str:
    persona = (
        "ä½ æ˜¯ä¸€ä½ã€æš–å¿ƒç™‚ç™’å¸«ã€â€” ä¸€ä½æº«æš–ã€å–„æ–¼å‚¾è½çš„å¥½æœ‹å‹ã€‚"
        "ä½ çš„ä¸»è¦ä»»å‹™æ˜¯é™ªä¼´å‰›å¤±æˆ€æˆ–å¿ƒæƒ…ä½è½çš„ä½¿ç”¨è€…ï¼Œä½†è«‹æ ¹æ“šä½¿ç”¨è€…çš„å°è©±å…§å®¹èª¿æ•´å›æ‡‰ã€‚"
    )
    instructions = (
        "å›ç­”æŒ‡å¼•ï¼š\n"
        "1. **åˆ¤æ–·æ„åœ–**ï¼š\n"
        "   - **å¦‚æœæ˜¯æ‰“æ‹›å‘¼**ï¼ˆå¦‚ã€Œä½ å¥½ã€ã€ã€Œæ—©å®‰ã€ï¼‰ï¼šè«‹è¦ªåˆ‡å›æ‡‰ä¸¦ç°¡å–®ä»‹ç´¹è‡ªå·±ï¼Œ**è«‹å‹¿**é è¨­å°æ–¹å·²ç¶“å¤±æˆ€ã€‚\n"
        "   - **å¦‚æœæ˜¯å‚¾è¨´ç…©æƒ±**ï¼šæ‰é–‹å§‹é‹ç”¨ä¸‹æ–¹çš„ã€åƒè€ƒè³‡æ–™ã€‘é€²è¡ŒåŒç†èˆ‡å»ºè­°ã€‚\n"
        "2. **å›æ‡‰åŸå‰‡**ï¼š\n"
        "   - å…ˆè‚¯å®šä¸¦æ¥ç´ä½¿ç”¨è€…çš„æƒ…ç·’ã€‚\n"
        "   - å¼•ç”¨åƒè€ƒè³‡æ–™ä¸­çš„å»ºè­°æ™‚ï¼Œè«‹è‡ªç„¶èå…¥å°è©±ã€‚\n"
        "   - ä¿æŒç°¡æ½”ã€æº«æŸ”ä¸”å¸¶æœ‰å¸Œæœ›ã€‚\n"
    )
    return f"{persona}\n\nã€åƒè€ƒè³‡æ–™ (Context)ã€‘:\n{context}\n\n{instructions}"


def main():
    st.set_page_config(page_title="æš–å¿ƒç™‚ç™’ Agent", page_icon="â¤ï¸â€ğŸ©¹")
    st.title("Heartbreak Healing Bot")
    st.subheader("å¤±æˆ€é™£ç·šè¯ç›Ÿé—œå¿ƒä½  æ‹’çµ•æˆ€æ„›è…¦å¤§ä½œæˆ°")

    api_key = load_env()

    # åˆå§‹åŒ–è³‡æ–™åº«
    if "vector_store" not in st.session_state:
        # å¦‚æœ books è³‡æ–™å¤¾å­˜åœ¨ä¸”æœ‰æª”æ¡ˆï¼Œæ‰å»ºç«‹
        if Path("books").exists() and list(Path("books").glob("*.txt")):
             with st.spinner("æ­£åœ¨é–±è®€ç™‚ç™’æ›¸ç±..."):
                st.session_state.vector_store = get_vector_store("books")
        else:
            st.session_state.vector_store = None

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # --- å´é‚Šæ¬„æ§åˆ¶ ---
    with st.sidebar:
        st.header("è¨­å®š")
        
        # æ¨¡å‹é¸æ“‡ (åŒ…å«ä½ å¸³è™Ÿå¯ç”¨çš„ 2.0 ç‰ˆæœ¬)
        model_name = st.selectbox(
            "Gemini æ¨¡å‹",
            options=["gemini-2.0-flash", "gemini-2.0-pro-exp", "gemini-1.5-pro"],
            index=0,
            help="Flash é€Ÿåº¦å¿«ï¼ŒPro é‚è¼¯å¼·ã€‚",
        )
        
        temperature = st.slider(
            "æ„Ÿæ€§ç¨‹åº¦ (Temperature)",
            0.0, 1.0, 0.7, 0.05,
            help="èª¿é«˜æœƒæ›´æº«æš–æ„Ÿæ€§ï¼Œèª¿ä½æœƒæ›´ç†æ€§ã€‚"
        )
        st.caption("ğŸ’¡ æç¤ºï¼šæ•¸å€¼è¶Šé«˜ï¼Œå›æ‡‰è¶Šæº«æš–æ„Ÿæ€§ã€‚")
        
        st.divider()
        if st.button("é‡å»ºçŸ¥è­˜åº« (Rebuild)"):
            with st.spinner("æ­£åœ¨é‡æ–°é–±è®€ä¸¦æ•´ç†è¨˜æ†¶..."):
                try:
                    if Path(".faiss_index").exists():
                        rmtree(Path(".faiss_index"))
                except Exception:
                    pass
                st.session_state.vector_store = get_vector_store("books")
            st.success("çŸ¥è­˜åº«æ›´æ–°å®Œæˆï¼")

    # æª¢æŸ¥è³‡æ–™åº«ç‹€æ…‹
    if not st.session_state.vector_store:
        st.info("ğŸ‘ˆ è«‹åœ¨ `books` è³‡æ–™å¤¾æ”¾å…¥ .txt æ–‡ç« ï¼Œä¸¦é»æ“Šå´é‚Šæ¬„çš„ã€Œé‡å»ºçŸ¥è­˜åº«ã€ã€‚")
        return

    # --- èŠå¤©è¦–çª— ---
    user_input = st.chat_input("æƒ³èªªä»€éº¼éƒ½å¯ä»¥ï¼Œæˆ‘åœ¨é€™è£¡é™ªä½ ...")

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # æª¢ç´¢
        retriever = st.session_state.vector_store.as_retriever(search_kwargs={"k": 4})
        with st.spinner("æ­£åœ¨å°‹æ‰¾æº«æš–çš„å»ºè­°..."):
            docs = retriever.invoke(user_input)
        
        context_text = "\n\n".join(d.page_content for d in docs)
        
        # æ•´ç†è³‡æ–™ä¾†æº (å»é™¤é‡è¤‡)
        sources = sorted(set(
            (d.metadata.get("source") or "æœªçŸ¥ä¾†æº").split("\\")[-1].split("/")[-1] 
            for d in docs
        ))

        system_prompt = build_persona_prompt(context_text)

        if not api_key:
            st.error("è«‹è¨­å®š API Keyã€‚")
            return

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_input),
        ]

        # ç”Ÿæˆ
        with st.chat_message("assistant"):
            with st.spinner("æ­£åœ¨ç”¨å¿ƒæ’°å¯«å›æ‡‰..."):
                try:
                    if _HAS_LC_GOOGLE:
                        llm = ChatGoogleGenerativeAI(
                            model=model_name,
                            google_api_key=api_key,
                            temperature=temperature,
                        )
                        response = llm.invoke(messages)
                        reply_text = getattr(response, "content", str(response))
                    else:
                        genai.configure(api_key=api_key)
                        gmodel = genai.GenerativeModel(model_name)
                        fallback_prompt = system_prompt + "\n\nä½¿ç”¨è€…ï¼š\n" + user_input
                        response = gmodel.generate_content(
                            fallback_prompt,
                            generation_config={"temperature": temperature},
                        )
                        reply_text = getattr(response, "text", str(response))
                    
                    # é¡¯ç¤ºè³‡æ–™ä¾†æº
                    if sources:
                        reply_text += "\n\n---\nğŸ“š **åƒè€ƒè³‡æ–™**: " + ", ".join(sources)
                    
                    st.markdown(reply_text)
                    st.session_state.messages.append({"role": "assistant", "content": reply_text})
                
                except Exception as e:
                    err_msg = str(e)
                    if not _HAS_LC_GOOGLE and _LC_GOOGLE_ERR:
                        err_msg += f"\nImport error: {_LC_GOOGLE_ERR}"
                    st.error(f"ç™¼ç”ŸéŒ¯èª¤: {err_msg}")

if __name__ == "__main__":
    main()