"""Heartbreak Healing Bot - Final simplified version
Uses Gemini 2.0 models (flash / pro-exp) and FAISS RAG.
"""

import os
from pathlib import Path
import streamlit as st
from dotenv import load_dotenv

try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    from langchain.text_splitter import RecursiveCharacterTextSplitter

try:
    from langchain_core.messages import HumanMessage, SystemMessage
except ImportError:
    from langchain.schema import HumanMessage, SystemMessage

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
try:
    from langchain_google_genai import ChatGoogleGenerativeAI
    _LC_GOOGLE_AVAILABLE = True
    _LC_GOOGLE_ERR = None
except Exception as _e:
    _LC_GOOGLE_AVAILABLE = False
    _LC_GOOGLE_ERR = str(_e)
    import google.generativeai as genai


def load_env() -> str:
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY", "")
    if api_key:
        os.environ["GOOGLE_API_KEY"] = api_key  # keep downstream libs happy
    if not api_key:
        st.warning("ç¼ºå°‘ GOOGLE_API_KEYï¼Œè«‹åœ¨ .env æˆ– Streamlit secrets ä¸­è¨­å®šã€‚", icon="âš ï¸")
    return api_key


def get_vector_store(books_dir: str = "books", cache_dir: str = ".faiss_index") -> FAISS:
    base_path = Path(books_dir)
    base_path.mkdir(parents=True, exist_ok=True)

    cache_path = Path(cache_dir)
    if cache_path.exists():
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={"device": "cpu"},
            )
            return FAISS.load_local(str(cache_path), embeddings, allow_dangerous_deserialization=True)
        except Exception:
            pass

    loader = DirectoryLoader(
        str(base_path),
        glob="**/*.txt",
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8"},
        show_progress=True,
    )
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = splitter.split_documents(docs)
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
    )
    vs = FAISS.from_documents(chunks, embeddings)
    cache_path.mkdir(parents=True, exist_ok=True)
    vs.save_local(str(cache_path))
    return vs


def build_persona_prompt(context: str) -> str:
    persona = (
        "ä½ æ˜¯ä¸€ä½ã€æš–å¿ƒç™‚ç™’å¤¥ä¼´ã€ï¼Œæº«æŸ”ä¸”å……æ»¿åŒç†å¿ƒã€‚"
        "è«‹å…ˆè‚¯å®šèˆ‡æ¥ä½ä½¿ç”¨è€…çš„æ„Ÿå—ï¼Œä¸èªªæ•™ã€ä¸æ‰¹åˆ¤ã€‚"
        "æ ¹æ“šæä¾›çš„å…§å®¹ï¼Œç”¨è²¼å¿ƒã€ç°¡æ½”çš„èªæ°£çµ¦å‡º 1â€“3 å€‹å°å»ºè­°ã€‚"
        "èªèª¿æº«æš–ã€æ”¯æŒèˆ‡é¼“å‹µã€‚"
    )
    instructions = (
        "å›è¦†åŸå‰‡:\n"
        "- å…ˆåŒç†èˆ‡è‚¯å®šæƒ…ç·’ (ä¾‹å¦‚ï¼šæˆ‘èƒ½æ„Ÿå—åˆ°ä½ ç¾åœ¨å¾ˆé›£å—)ã€‚\n"
        "- æº«æŸ”åœ°åæ˜ ä½¿ç”¨è€…çš„å¿ƒæƒ…ã€‚\n"
        "- å†çµ¦å‡º 1â€“3 å€‹å¯è¡Œçš„å°æ­¥é©Ÿæˆ–è‡ªæˆ‘é—œæ‡·å»ºè­°ã€‚\n"
        "- å¥å­ä¿æŒç°¡æ½”ï¼Œå¸Œæœ›ã€æ”¯æŒã€‚\n"
        "- ä¸è¦é†«ç™‚è¨ºæ–·æˆ–æ‰¹åˆ¤ã€‚"
    )
    return f"{persona}\n\nåƒè€ƒå…§å®¹:\n{context}\n\n{instructions}"


def main():
    st.set_page_config(page_title="Heartbreak Healing Bot", page_icon="ğŸ’—")
    st.title("ğŸ’— Heartbreak Healing Bot")
    st.subheader("å¤±æˆ€é™£ç·šè¯ç›Ÿé—œå¿ƒä½  æ‹’çµ•æˆ€æ„›è…¦å¤§ä½œæˆ°")
    # st.caption("æº«æŸ”çš„ RAG åŠ©ç†ï¼Œæ¡ç”¨ Gemini 2.0ã€‚")

    api_key = load_env()

    if "vector_store" not in st.session_state:
        with st.spinner("Building vector store â€¦"):
            st.session_state.vector_store = get_vector_store("books")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    with st.sidebar:
        st.header("è¨­å®š")
        model_label = "Gemini 2.0 æ¨¡å‹"
        model_help = "é¸æ“‡è¼ƒå¿« (flash) æˆ–è¼ƒè°æ˜å¯¦é©—ç‰ˆ (pro-exp)"
        model_name = st.selectbox(
            model_label,
            options=["gemini-2.0-flash", "gemini-2.0-pro-exp"],
            index=0,
            help=model_help,
        )
        temp_label = "å‰µé€ åŠ›ï¼ˆTemperatureï¼‰"
        temp_help = "ä½ = ç†æ€§ã€ é«˜ = æº«æš–æƒ…æ„Ÿ"
        temperature = st.slider(temp_label, 0.0, 1.0, 0.7, 0.05, help=temp_help)
        st.caption("è¼ƒä½åç†æ€§å»ºè­°ï¼Œè¼ƒé«˜åæº«æš–æƒ…æ„Ÿé™ªä¼´ã€‚")
        if st.button("é‡å»ºå‘é‡è³‡æ–™åº«"):
            with st.spinner("é‡å»ºä¸­â€¦"):
                try:
                    from shutil import rmtree
                    rmtree(Path(".faiss_index"))
                except Exception:
                    pass
                st.session_state.vector_store = get_vector_store("books")
            st.success("é‡å»ºå®Œæˆï¼")

    placeholder_input = "æƒ³èªªä»€éº¼éƒ½å¯ä»¥â€¦æˆ‘åœ¨é€™è£¡é™ªä½ "
    user_input = st.chat_input(placeholder_input)

    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        retriever = st.session_state.vector_store.as_retriever(search_kwargs={"k": 4})
        with st.spinner("æª¢ç´¢æ”¯æŒæ€§å…§å®¹ä¸­â€¦"):
            try:
                docs = retriever.get_relevant_documents(user_input)
            except AttributeError:
                docs = retriever.invoke(user_input)
        context_text = "\n\n".join(d.page_content for d in docs)

        system_prompt = build_persona_prompt(context_text)

        if not api_key:
            st.error(
                "ç¼ºå°‘ GOOGLE_API_KEYï¼Œè«‹åœ¨ .env ä¸­è¨­å®šã€‚"
            )
            return

        messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_input)]

        with st.spinner("æº«æŸ”æ’°å¯«å›è¦†ä¸­â€¦"):
            if _LC_GOOGLE_AVAILABLE:
                try:
                    llm = ChatGoogleGenerativeAI(
                        model=model_name,
                        google_api_key=api_key,
                        temperature=temperature,
                    )
                    resp = llm.invoke(messages)
                    reply = getattr(resp, "content", str(resp))
                except Exception as e:
                    st.error(f"Model call failed: {e}")
                    return
            else:
                try:
                    genai.configure(api_key=api_key)
                    gmodel = genai.GenerativeModel(model_name)
                    fallback_prompt = system_prompt + "\n\nä½¿ç”¨è€…ï¼š\n" + user_input
                    response = gmodel.generate_content(fallback_prompt, generation_config={"temperature": temperature})
                    reply = getattr(response, "text", str(response))
                except Exception as e:
                    st.error(f"å‚™æ´ Gemini å‘¼å«å¤±æ•—: {e}\nImport error: {_LC_GOOGLE_ERR}")
                    return

        st.session_state.messages.append({"role": "assistant", "content": reply})
        with st.chat_message("assistant"):
            st.markdown(reply)


if __name__ == "__main__":
    main()
